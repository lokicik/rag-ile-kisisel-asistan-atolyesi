{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RLULMPXa-Hu8",
        "_IlznUDK-i2m",
        "2LPwdGDP-nPO",
        "U_nH1qoL-w--",
        "5_aXp4WcWBQN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain ile KiÅŸiye Ã–zel Chatbot - PDF'lerinizle konuÅŸun!**\n",
        "\n",
        "\n",
        "**Notebook'u hazÄ±rlayan: [Lokman Baturay Efe](https://www.linkedin.com/in/lokmanefe/)**\n",
        "\n",
        "\n",
        "0. Ä°ndirmeler, KÃ¼tÃ¼phaneler and API AnahtarlarÄ±\n",
        "1. PDF'leri yÃ¼kleme ve LangChain ile parÃ§alara ayÄ±rma\n",
        "2. Metinleri gÃ¶mme (embedding) ve gÃ¶mme sonuÃ§larÄ±nÄ± depolama\n",
        "3. EriÅŸim (retrieval) fonksiyonu oluÅŸturma\n",
        "4. Sohbet hafÄ±zalÄ± bir sohbet botu oluÅŸturma\n",
        "5. TÃ¼m iÅŸlemi Gradio ile daha kullanÄ±labilir hale getirme\n",
        "\n",
        "**Notebook hazÄ±rlanÄ±rken [Liam Ottley](https://youtube.com/@LiamOttley)'in hazÄ±rladÄ±ÄŸÄ± [notebook](https://colab.research.google.com/drive/1OZpmLgd5D_qmjTnL5AsD1_ZJDdb7LQZI?usp=sharing) referans alÄ±nmÄ±ÅŸtÄ±r.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Ä°ndirmeler, KÃ¼tÃ¼phaneler and API AnahtarlarÄ±\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q24Y-g6h-Bg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip==24.0"
      ],
      "metadata": {
        "id": "BIeCYGz-FXem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf pandas matplotlib tiktoken transformers faiss-cpu langchain-community langchain-google-genai textract==1.6.5 gradio google-cloud-aiplatform[tokenization]"
      ],
      "metadata": {
        "id": "gk2J2sYYjTkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textract\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from vertexai.preview import tokenization\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "l-uszlwN641q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"API anahtarÄ±nÄ±zÄ± girin.\"\n",
        "\n",
        "file_name = \"PDF dosya ismini girin.\""
      ],
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PDF'leri yÃ¼kleme ve LangChain ile parÃ§alara ayÄ±rma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RLULMPXa-Hu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ã–ncelikle kullanacaÄŸÄ±mÄ±z PDF dosyalarÄ±nÄ± Colab'in dosyalar kÄ±smÄ±na eklememiz gerekiyor!\n",
        "\n",
        "# PDF'i hafÄ±zaya yÃ¼klÃ¼yoruz.\n",
        "loader = PyPDFLoader(f\"./{file_name}.pdf\")\n",
        "\n",
        "# PDF'i okuyup sayfalara ayÄ±rÄ±yoruz.\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0])"
      ],
      "metadata": {
        "id": "KH546j3nkFwX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = pages\n",
        "# AdÄ±m 1: PDF dosyasÄ±nÄ± metne Ã§eviriyoruz.\n",
        "doc = textract.process(f\"./{file_name}.pdf\")\n",
        "\n",
        "# AdÄ±m 2: Hatalara engel olmasÄ± iÃ§in .txt olarak kaydedip tekrar okuyoruz.\n",
        "with open(f\"./{file_name}.txt\", 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open(f\"./{file_name}.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# AdÄ±m 3: Tokenleri saymak iÃ§in bir fonksiyon oluÅŸturuyoruz.\n",
        "tokenizer = tokenization.get_tokenizer_for_model(\"gemini-1.5-flash\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    result = tokenizer.count_tokens(text)\n",
        "    return result.total_tokens\n",
        "\n",
        "# AdÄ±m 4: Metinleri parÃ§alara ayÄ±rÄ±yoruz.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 24,\n",
        "    length_function = count_tokens,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "iADY2CXNlNq9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OluÅŸan sonuÃ§lar her biri yaklaÅŸÄ±k 500 token veya 500'den daha az token\n",
        "# iÃ§eren LangChain Document nesneleridir. (RecursiveCharacterTextSplitter yÃ¼zÃ¼nden\n",
        "# token sayÄ±sÄ± 500'den fazla da olabilir, eÄŸer baÄŸlamÄ± koparmak kÃ¶tÃ¼yse 500'Ã¼ aÅŸabilir)\n",
        "type(chunks[0])"
      ],
      "metadata": {
        "id": "KQ_gDkwep4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ParÃ§alara ayÄ±rma iÅŸleminin baÅŸarÄ±lÄ± olup olmadÄ±ÄŸÄ±nÄ± anlamak iÃ§in bir gÃ¶rselleÅŸtirme yapalÄ±m.\n",
        "\n",
        "# Her bir parÃ§adaki token sayÄ±sÄ±nÄ±n bir listesini oluÅŸturalÄ±m.\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# OluÅŸturduÄŸumuz listeden bir veri tablosu oluÅŸturalÄ±m.\n",
        "df = pd.DataFrame({'Token Sayisi': token_counts})\n",
        "\n",
        "# Token sayÄ±sÄ± daÄŸÄ±lÄ±mÄ±nÄ± incelemek iÃ§in bir histogram oluÅŸturalÄ±m.\n",
        "df.hist(bins=40, )\n",
        "\n",
        "# OluÅŸturduÄŸumuz grafiÄŸi gÃ¶sterelim.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fK31bxDOpz1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Metinleri gÃ¶mme (embedding) ve gÃ¶mme sonuÃ§larÄ±nÄ± depolama\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_IlznUDK-i2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding (metin gÃ¶mme) modelini Ã§aÄŸÄ±ralÄ±m.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
        "\n",
        "# OluÅŸturduÄŸumuz embeddinglerden bir vektÃ¶r veritabanÄ± oluÅŸturalÄ±m.\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "92ObhTAKnZzQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. EriÅŸim (retrieval) fonksiyonu oluÅŸturma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2LPwdGDP-nPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benzerlik algoritmasÄ±nÄ±n doÄŸru sonuÃ§ verip vermediÄŸini test edelim.\n",
        "query = \"RAG ile KiÅŸisel Asistan eÄŸitimi\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "RWP92zGg5Nb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KullanÄ±cÄ±nÄ±n girdileri ile arama yapmak iÃ§in benzerlik algoritmasÄ±nÄ± kullanarak\n",
        "# bir soru cevap zinciri oluÅŸturuyoruz. (KullanÄ±cÄ±nÄ±n girdisine artÄ±k verilen baÄŸlama bakarak cevap verecek.)\n",
        "\n",
        "chain = load_qa_chain(\n",
        "    ChatGoogleGenerativeAI(\n",
        "        temperature=0.7,\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "        model=\"gemini-1.5-flash\"),\n",
        "    chain_type=\"stuff\"\n",
        ")"
      ],
      "metadata": {
        "id": "1Kv_sM8G5qAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OluÅŸturduÄŸumuz soru cevap zincirini Ã§alÄ±ÅŸtÄ±rarak dokÃ¼manlar Ã¼zerinde test edelim.\n",
        "query = \"RAG ile KiÅŸisel Asistan eÄŸitimini kim veriyor?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "y2zcWrK-Wyij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sohbet hafÄ±zalÄ± bir sohbet botu oluÅŸturma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U_nH1qoL-w--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Daha Ã¶nce FAISS kullanarak oluÅŸturduÄŸumuz vektÃ¶r veritabanÄ±nÄ±\n",
        "# dokÃ¼manlarÄ±mÄ±za eriÅŸim mekanizmasÄ± olarak kullanarak\n",
        "# hafÄ±zaya sahip olan (Ã¶nceki mesajlarÄ± hatÄ±rlayabilen) bir konuÅŸma zinciri oluÅŸturalÄ±m.\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatGoogleGenerativeAI(temperature=0.7, api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"gemini-1.5-flash\"), db.as_retriever())"
      ],
      "metadata": {
        "id": "evF7_Dyhtcaf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"GDG On Campus Trakya Chatbot'unu kullandÄ±ÄŸÄ±nÄ±z iÃ§in teÅŸekkÃ¼rler!\")\n",
        "        return\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"GDG On Campus Trakya Chatbot'una hoÅŸgeldiniz!\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='LÃ¼tfen sorunuzu girin:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ],
      "metadata": {
        "id": "-pHw5siewPNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. TÃ¼m iÅŸlemi Gradio ile daha kullanÄ±labilir hale getirme\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5_aXp4WcWBQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token sayma fonksiyonu\n",
        "tokenizer = tokenization.get_tokenizer_for_model(\"gemini-1.5-flash\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    result = tokenizer.count_tokens(text)\n",
        "    return result.total_tokens\n",
        "\n",
        "\n",
        "# Chatbot fonksiyonu\n",
        "def chatbot(api_key, uploaded_file, chat_history, query):\n",
        "    os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "\n",
        "    # PDF'i hafÄ±zaya yÃ¼klÃ¼yoruz\n",
        "    loader = PyPDFLoader(uploaded_file.name)\n",
        "    pages = loader.load_and_split()\n",
        "\n",
        "    # Metni iÅŸliyoruz\n",
        "    doc = textract.process(uploaded_file.name)\n",
        "    text = doc.decode('utf-8')\n",
        "\n",
        "    # Metinleri parÃ§alara ayÄ±rma\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "    chunks = text_splitter.create_documents([text])\n",
        "\n",
        "    # Embedding iÅŸlemi\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(\n",
        "        google_api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "        model=\"models/embedding-001\"\n",
        "    )\n",
        "\n",
        "    # VektÃ¶r veritabanÄ± oluÅŸturma\n",
        "    db = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    # KonuÅŸma zinciri oluÅŸturma\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        ChatGoogleGenerativeAI(\n",
        "            temperature=0.7,\n",
        "            api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "            model=\"gemini-1.5-flash\"\n",
        "        ),\n",
        "        db.as_retriever()\n",
        "    )\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    answer = result['answer']\n",
        "    chat_history.append((query, answer))\n",
        "    return chat_history, chat_history\n",
        "\n",
        "# Gradio arayÃ¼zÃ¼\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ğŸ¦œğŸ”—LangChain ile KiÅŸiye Ã–zel Chatbot - PDF'lerinizle KonuÅŸun!\")\n",
        "\n",
        "    with gr.Column():\n",
        "        api_key = gr.Textbox(label=\"LÃ¼tfen GEMINI API anahtarÄ±nÄ±zÄ± girin:\", type=\"password\")\n",
        "        uploaded_file = gr.File(label=\"LÃ¼tfen bir PDF dosyasÄ± yÃ¼kleyin\", file_types=[\".pdf\"])\n",
        "        chatbot_interface = gr.Chatbot()\n",
        "        query = gr.Textbox(label=\"LÃ¼tfen sorunuzu girin:\")\n",
        "\n",
        "    chat_history = gr.State([])\n",
        "\n",
        "    def respond(api_key, uploaded_file, chat_history, query):\n",
        "        chat_history, _ = chatbot(api_key, uploaded_file, chat_history, query)\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    query.submit(respond, [api_key, uploaded_file, chat_history, query], [chatbot_interface, query])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "61dBMLCSTEbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}