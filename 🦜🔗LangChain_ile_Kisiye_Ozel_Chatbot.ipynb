{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RLULMPXa-Hu8",
        "_IlznUDK-i2m",
        "2LPwdGDP-nPO",
        "U_nH1qoL-w--",
        "5_aXp4WcWBQN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain ile Kişiye Özel Chatbot - PDF'lerinizle konuşun!**\n",
        "\n",
        "\n",
        "**Notebook'u hazırlayan: [Lokman Baturay Efe](https://www.linkedin.com/in/lokmanefe/)**\n",
        "\n",
        "\n",
        "0. İndirmeler, Kütüphaneler and API Anahtarları\n",
        "1. PDF'leri yükleme ve LangChain ile parçalara ayırma\n",
        "2. Metinleri gömme (embedding) ve gömme sonuçlarını depolama\n",
        "3. Erişim (retrieval) fonksiyonu oluşturma\n",
        "4. Sohbet hafızalı bir sohbet botu oluşturma\n",
        "5. Tüm işlemi Gradio ile daha kullanılabilir hale getirme\n",
        "\n",
        "**Notebook hazırlanırken [Liam Ottley](https://youtube.com/@LiamOttley)'in hazırladığı [notebook](https://colab.research.google.com/drive/1OZpmLgd5D_qmjTnL5AsD1_ZJDdb7LQZI?usp=sharing) referans alınmıştır.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. İndirmeler, Kütüphaneler and API Anahtarları\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q24Y-g6h-Bg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip==24.0"
      ],
      "metadata": {
        "id": "BIeCYGz-FXem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf pandas matplotlib tiktoken transformers faiss-cpu langchain-community langchain-google-genai textract==1.6.5 gradio google-cloud-aiplatform[tokenization]"
      ],
      "metadata": {
        "id": "gk2J2sYYjTkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textract\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from vertexai.preview import tokenization\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "l-uszlwN641q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"API anahtarınızı girin.\"\n",
        "\n",
        "file_name = \"PDF dosya ismini girin.\""
      ],
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PDF'leri yükleme ve LangChain ile parçalara ayırma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RLULMPXa-Hu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Öncelikle kullanacağımız PDF dosyalarını Colab'in dosyalar kısmına eklememiz gerekiyor!\n",
        "\n",
        "# PDF'i hafızaya yüklüyoruz.\n",
        "loader = PyPDFLoader(f\"./{file_name}.pdf\")\n",
        "\n",
        "# PDF'i okuyup sayfalara ayırıyoruz.\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0])"
      ],
      "metadata": {
        "id": "KH546j3nkFwX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = pages\n",
        "# Adım 1: PDF dosyasını metne çeviriyoruz.\n",
        "doc = textract.process(f\"./{file_name}.pdf\")\n",
        "\n",
        "# Adım 2: Hatalara engel olması için .txt olarak kaydedip tekrar okuyoruz.\n",
        "with open(f\"./{file_name}.txt\", 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open(f\"./{file_name}.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Adım 3: Tokenleri saymak için bir fonksiyon oluşturuyoruz.\n",
        "tokenizer = tokenization.get_tokenizer_for_model(\"gemini-1.5-flash\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    result = tokenizer.count_tokens(text)\n",
        "    return result.total_tokens\n",
        "\n",
        "# Adım 4: Metinleri parçalara ayırıyoruz.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 24,\n",
        "    length_function = count_tokens,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "iADY2CXNlNq9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oluşan sonuçlar her biri yaklaşık 500 token veya 500'den daha az token\n",
        "# içeren LangChain Document nesneleridir. (RecursiveCharacterTextSplitter yüzünden\n",
        "# token sayısı 500'den fazla da olabilir, eğer bağlamı koparmak kötüyse 500'ü aşabilir)\n",
        "type(chunks[0])"
      ],
      "metadata": {
        "id": "KQ_gDkwep4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parçalara ayırma işleminin başarılı olup olmadığını anlamak için bir görselleştirme yapalım.\n",
        "\n",
        "# Her bir parçadaki token sayısının bir listesini oluşturalım.\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# Oluşturduğumuz listeden bir veri tablosu oluşturalım.\n",
        "df = pd.DataFrame({'Token Sayisi': token_counts})\n",
        "\n",
        "# Token sayısı dağılımını incelemek için bir histogram oluşturalım.\n",
        "df.hist(bins=40, )\n",
        "\n",
        "# Oluşturduğumuz grafiği gösterelim.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fK31bxDOpz1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Metinleri gömme (embedding) ve gömme sonuçlarını depolama\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_IlznUDK-i2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding (metin gömme) modelini çağıralım.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
        "\n",
        "# Oluşturduğumuz embeddinglerden bir vektör veritabanı oluşturalım.\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "92ObhTAKnZzQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Erişim (retrieval) fonksiyonu oluşturma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2LPwdGDP-nPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benzerlik algoritmasının doğru sonuç verip vermediğini test edelim.\n",
        "query = \"RAG ile Kişisel Asistan eğitimi\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "RWP92zGg5Nb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kullanıcının girdileri ile arama yapmak için benzerlik algoritmasını kullanarak\n",
        "# bir soru cevap zinciri oluşturuyoruz. (Kullanıcının girdisine artık verilen bağlama bakarak cevap verecek.)\n",
        "\n",
        "chain = load_qa_chain(\n",
        "    ChatGoogleGenerativeAI(\n",
        "        temperature=0.7,\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "        model=\"gemini-1.5-flash\"),\n",
        "    chain_type=\"stuff\"\n",
        ")"
      ],
      "metadata": {
        "id": "1Kv_sM8G5qAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oluşturduğumuz soru cevap zincirini çalıştırarak dokümanlar üzerinde test edelim.\n",
        "query = \"RAG ile Kişisel Asistan eğitimini kim veriyor?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "y2zcWrK-Wyij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sohbet hafızalı bir sohbet botu oluşturma\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U_nH1qoL-w--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Daha önce FAISS kullanarak oluşturduğumuz vektör veritabanını\n",
        "# dokümanlarımıza erişim mekanizması olarak kullanarak\n",
        "# hafızaya sahip olan (önceki mesajları hatırlayabilen) bir konuşma zinciri oluşturalım.\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatGoogleGenerativeAI(temperature=0.7, api_key=os.environ.get(\"GEMINI_API_KEY\"), model=\"gemini-1.5-flash\"), db.as_retriever())"
      ],
      "metadata": {
        "id": "evF7_Dyhtcaf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"GDG On Campus Trakya Chatbot'unu kullandığınız için teşekkürler!\")\n",
        "        return\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"GDG On Campus Trakya Chatbot'una hoşgeldiniz!\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Lütfen sorunuzu girin:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ],
      "metadata": {
        "id": "-pHw5siewPNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tüm işlemi Gradio ile daha kullanılabilir hale getirme\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5_aXp4WcWBQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token sayma fonksiyonu\n",
        "tokenizer = tokenization.get_tokenizer_for_model(\"gemini-1.5-flash\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    result = tokenizer.count_tokens(text)\n",
        "    return result.total_tokens\n",
        "\n",
        "\n",
        "# Chatbot fonksiyonu\n",
        "def chatbot(api_key, uploaded_file, chat_history, query):\n",
        "    os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "\n",
        "    # PDF'i hafızaya yüklüyoruz\n",
        "    loader = PyPDFLoader(uploaded_file.name)\n",
        "    pages = loader.load_and_split()\n",
        "\n",
        "    # Metni işliyoruz\n",
        "    doc = textract.process(uploaded_file.name)\n",
        "    text = doc.decode('utf-8')\n",
        "\n",
        "    # Metinleri parçalara ayırma\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=24,\n",
        "        length_function=count_tokens,\n",
        "    )\n",
        "    chunks = text_splitter.create_documents([text])\n",
        "\n",
        "    # Embedding işlemi\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(\n",
        "        google_api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "        model=\"models/embedding-001\"\n",
        "    )\n",
        "\n",
        "    # Vektör veritabanı oluşturma\n",
        "    db = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    # Konuşma zinciri oluşturma\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        ChatGoogleGenerativeAI(\n",
        "            temperature=0.7,\n",
        "            api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "            model=\"gemini-1.5-flash\"\n",
        "        ),\n",
        "        db.as_retriever()\n",
        "    )\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    answer = result['answer']\n",
        "    chat_history.append((query, answer))\n",
        "    return chat_history, chat_history\n",
        "\n",
        "# Gradio arayüzü\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🦜🔗LangChain ile Kişiye Özel Chatbot - PDF'lerinizle Konuşun!\")\n",
        "\n",
        "    with gr.Column():\n",
        "        api_key = gr.Textbox(label=\"Lütfen GEMINI API anahtarınızı girin:\", type=\"password\")\n",
        "        uploaded_file = gr.File(label=\"Lütfen bir PDF dosyası yükleyin\", file_types=[\".pdf\"])\n",
        "        chatbot_interface = gr.Chatbot()\n",
        "        query = gr.Textbox(label=\"Lütfen sorunuzu girin:\")\n",
        "\n",
        "    chat_history = gr.State([])\n",
        "\n",
        "    def respond(api_key, uploaded_file, chat_history, query):\n",
        "        chat_history, _ = chatbot(api_key, uploaded_file, chat_history, query)\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    query.submit(respond, [api_key, uploaded_file, chat_history, query], [chatbot_interface, query])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "61dBMLCSTEbi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}